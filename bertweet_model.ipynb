{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run only once ~8min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertweet = AutoModel.from_pretrained('vinai/bertweet-large')\n",
    "tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-large', use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = 'saved'\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "bertweet.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of using bertweet and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Bertweet base model\n",
    "save_directory = 'saved'\n",
    "bertweet = AutoModel.from_pretrained(save_directory)\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 31414, 232, 6, 42, 16, 127, 78, 86, 634, 12975, 21210, 328, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example string input to tokenizer\n",
    "line = \"Hello world, this is my first time using Bertweet!\"\n",
    "res = tokenizer(line)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   0,  713,   16,  127,   78, 3645,    4,    2,    1,    1,    1,    1,\n",
       "            1,    1,    1],\n",
       "        [   0, 2409,   42,   16,  127,  200, 3645,    6,   38, 1034,   47, 1346,\n",
       "           24,    4,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating batch for input into Bertweet\n",
    "X_train = ['This is my first sentence.', 'And this is my second sentence, I hope you understand it.']\n",
    "batch = tokenizer(X_train, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0453, -0.0353,  0.1269,  ...,  0.1318,  0.1431, -0.0268],\n",
      "         [ 0.1216, -0.1689, -0.8237,  ...,  0.3062, -0.3307,  0.3082],\n",
      "         [ 0.0916,  0.1439, -0.9215,  ...,  0.3699, -0.0623,  0.2387],\n",
      "         ...,\n",
      "         [-0.0136, -0.0934, -0.3526,  ...,  0.0756,  0.0351,  0.0351],\n",
      "         [-0.0136, -0.0934, -0.3526,  ...,  0.0756,  0.0351,  0.0351],\n",
      "         [-0.0136, -0.0934, -0.3526,  ...,  0.0756,  0.0351,  0.0351]],\n",
      "\n",
      "        [[-0.2916, -0.2605,  0.1840,  ...,  0.2022, -0.0896,  0.2360],\n",
      "         [ 0.0496,  0.3750, -0.7756,  ..., -0.0128,  0.2076,  0.1194],\n",
      "         [ 0.0489, -0.4376, -1.0619,  ...,  0.2605, -0.5458,  0.1574],\n",
      "         ...,\n",
      "         [-0.0306, -0.0894, -0.0339,  ..., -0.1659,  0.0283, -0.0898],\n",
      "         [ 0.0696,  0.0282,  0.0196,  ..., -0.0197,  0.2267, -0.0093],\n",
      "         [ 0.0338, -0.0204, -0.0525,  ..., -0.0569,  0.1598, -0.0067]]])\n",
      "torch.Size([2, 15, 1024])\n"
     ]
    }
   ],
   "source": [
    "# Run bertweet on one batch\n",
    "with torch.no_grad():\n",
    "    outputs = bertweet(**batch)\n",
    "print(outputs.last_hidden_state)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0, 31414,   232,     6,    42,    16,   127,    78,    86,   634,\n",
      "         12975, 21210,   328,     2]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0285,  0.0044, -0.0314,  ...,  0.0243,  0.0553,  0.0443],\n",
       "         [-0.3286,  0.6190, -1.5636,  ..., -0.1887, -0.4045,  0.0190],\n",
       "         [ 0.0090,  0.2959, -0.4406,  ...,  0.4447, -0.1542,  0.1028],\n",
       "         ...,\n",
       "         [-0.1125, -0.3202, -0.2327,  ...,  0.1368,  0.3177,  0.0216],\n",
       "         [-0.4010,  0.1004, -0.4539,  ...,  0.0684, -0.1501, -0.0831],\n",
       "         [-0.0237, -0.0087, -0.0484,  ...,  0.0073,  0.0453,  0.0352]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.2569,  0.4870, -0.4550,  ...,  0.3592, -0.4953, -0.7143]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternative way to create input to bertweet\n",
    "input_ids = torch.tensor([tokenizer.encode(line)])\n",
    "print(input_ids)\n",
    "bertweet(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize tokenizer for understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size of tweet tokenizer: 50265\n",
      "\n",
      "*** Example ***\n",
      "Example sentence: We have an example, but not a scarecrow.\n",
      "Tokenizer output: {'input_ids': [0, 170, 33, 41, 1246, 6, 53, 45, 10, 13207, 43520, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Tokens: ['<s>', 'We', ' have', ' an', ' example', ',', ' but', ' not', ' a', ' scare', 'crow', '.', '</s>']\n",
      "Reconstructed sentence <s>We have an example, but not a scarecrow.</s>\n"
     ]
    }
   ],
   "source": [
    "# Source: A4\n",
    "# Load tokenizer from saved directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "print(\"Vocab size of tweet tokenizer:\", tokenizer.vocab_size)\n",
    "\n",
    "# As a demonstration, we will show \n",
    "example_sentence = \"We have an example, but not a scarecrow.\"\n",
    "tokenizer_output = tokenizer(example_sentence)\n",
    "print(\"\\n*** Example ***\")\n",
    "print(\"Example sentence:\", example_sentence)\n",
    "print(\"Tokenizer output:\", tokenizer_output)\n",
    "\n",
    "# We convert every token id to its associated token string.\n",
    "# Note that very common words are represented by a single token, while others are split into subunits due to the small vocab size.\n",
    "# Also note that †he tokenizer already adds special tokens to the beginning and end of the sentence.\n",
    "decoded_sequence = [tokenizer.decode(token) for token in tokenizer_output[\"input_ids\"]]\n",
    "print(\"Tokens:\", decoded_sequence)\n",
    "\n",
    "# By replacing the special character ▁ with whitespace, we can reconstruct a legibile sentence,\n",
    "# which differs from the original example by special tokens, includings <unk> tokens, and minor whitespace differences.\n",
    "reconstructed = \"\".join(decoded_sequence)\n",
    "print(\"Reconstructed sentence\", reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note above is that \"We\", \"we\", and \" we\" will all give different token ids for we. It seems to differentiate between words that start a sentence and words that are in the middle of a sentence. It also differentiates between words that start with capital letters and words with lowercase letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "def preprocess_function(example: Dict[str, List[int]]) -> Dict[str, List[int]]:\n",
    "    # Tokenize the input text\n",
    "    tokenized_input = tokenizer(example[\"text\"])\n",
    "\n",
    "    # Return a dictionary with both input_ids and the corresponding label\n",
    "    return {\n",
    "        \"input_ids\": tokenized_input[\"input_ids\"],\n",
    "        \"label\": example[\"label\"]\n",
    "    }\n",
    "\n",
    "# When mapped is applied to the DatasetDict object, it will apply `map` separately to each split.\n",
    "# map() is from datasets library\n",
    "tokenized_datasets = insert_raw_text_datasets_here.map(preprocess_function, batched=False)\n",
    "\n",
    "# # POSSIBLY UNNECESSARY\n",
    "# # The `remove_columns` removes the existing text features from the new dataset, as they are no longer needed.\n",
    "# tokenized_datasets = tokenized_datasets.remove_columns(insert_raw_text_datasets_here.column_names[\"train\"])\n",
    "\n",
    "# # POSSIBLY UNNECESSARY\n",
    "# # Sanity checks on the new dataset\n",
    "# assert set(tokenized_datasets.column_names[\"train\"]) == {\"input_ids\", \"label\"}\n",
    "# assert len(tokenized_datasets[\"train\"]) == len(insert_raw_text_datasets_here[\"train\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
